{
"cells": [
{
"cell_type": "code",
"execution_count": null,
"id": "checkpoint_inference",
"metadata": {},
"outputs": [],
"source": [
"import io, zipfile, random, importlib, traceback\n",
"import urllib.request\n",
"import cv2\n",
"import torch\n",
"import numpy as np\n",
"import matplotlib.pyplot as plt\n",
"import segmentation_models_pytorch as smp\n",
"\n",
"# -----------------------\n",
"# RAW URLs\n",
"# -----------------------\n",
"WEIGHTS_URL = "[https://github.com/sequeiradiogo/Building-data-extraction/raw/main/models/unet_resnet34_epoch1_IoU0.7833.pth"\n](https://github.com/sequeiradiogo/Building-data-extraction/raw/main/models/unet_resnet34_epoch1_IoU0.7833.pth%22\n)",
"IMAGES_ZIP_URL = "[https://github.com/sequeiradiogo/Building-data-extraction/raw/main/output/examples/test_images.zip"\n](https://github.com/sequeiradiogo/Building-data-extraction/raw/main/output/examples/test_images.zip%22\n)",
"\n",
"MODEL_INPUT_SIZE = (512, 512)\n",
"THRESHOLD = 0.5\n",
"\n",
"# -----------------------\n",
"# Install dependencies\n",
"# -----------------------\n",
"!pip install -q segmentation-models-pytorch==0.3.0 torchvision torch opencv-python matplotlib pillow tqdm\n",
"\n",
"# -----------------------\n",
"# safe in-memory checkpoint loader\n",
"# -----------------------\n",
"def safe_load_checkpoint_from_bytes(weights_bytes, map_location):\n",
"    ckpt = None\n",
"    bio = io.BytesIO(weights_bytes)\n",
"    try:\n",
"        ckpt = torch.load(bio, map_location=map_location, weights_only=True)\n",
"        print("Loaded checkpoint with weights_only=True")\n",
"        return ckpt\n",
"    except Exception as e:\n",
"        print("weights_only=True failed:", e)\n",
"\n",
"    unsafe = []\n",
"    try:\n",
"        bio.seek(0)\n",
"        unsafe = torch.serialization.get_unsafe_globals_in_checkpoint(bio)\n",
"        print("Unsafe globals reported by checkpoint:", unsafe)\n",
"    except Exception as e2:\n",
"        print("Could not query unsafe globals:", e2)\n",
"        unsafe = []\n",
"\n",
"    safe_objs = []\n",
"    for full_name in unsafe:\n",
"        try:\n",
"            module_name, obj_name = full_name.rsplit(".", 1)\n",
"            mod = importlib.import_module(module_name)\n",
"            obj = getattr(mod, obj_name)\n",
"            safe_objs.append(obj)\n",
"            print("Will allowlist:", full_name)\n",
"        except Exception as imp_e:\n",
"            print(f"Could not import {full_name}: {imp_e}")\n",
"\n",
"    if safe_objs:\n",
"        try:\n",
"            bio.seek(0)\n",
"            torch.serialization.add_safe_globals(safe_objs)\n",
"            ckpt = torch.load(bio, map_location=map_location, weights_only=True)\n",
"            print("Loaded checkpoint after allowlisting unsafe globals (weights_only=True).")\n",
"            return ckpt\n",
"        except Exception as e3:\n",
"            print("Failed after allowlisting:", e3)\n",
"            traceback.print_exc()\n",
"            print("Falling back to trusted load (weights_only=False).")\n",
"\n",
"    try:\n",
"        bio.seek(0)\n",
"        ckpt = torch.load(bio, map_location=map_location, weights_only=False)\n",
"        print("Loaded checkpoint with weights_only=False (trusted load).")\n",
"        return ckpt\n",
"    except Exception as e_final:\n",
"        print("Trusted load also failed:", e_final)\n",
"        traceback.print_exc()\n",
"        raise RuntimeError("Could not load checkpoint: " + str(e_final))\n",
"\n",
"# -----------------------\n",
"# Download model weights into memory\n",
"# -----------------------\n",
"print("Downloading model weights...")\n",
"with urllib.request.urlopen(WEIGHTS_URL) as response:\n",
"    weights_bytes = response.read()\n",
"\n",
"device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n",
"\n",
"model = smp.Unet(\n",
"    encoder_name="resnet34",\n",
"    encoder_weights=None,\n",
"    in_channels=3,\n",
"    classes=1,\n",
")\n",
"\n",
"# -----------------------\n",
"# Load weights robustly\n",
"# -----------------------\n",
"ckpt = safe_load_checkpoint_from_bytes(weights_bytes, map_location=device)\n",
"\n",
"if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:\n",
"    model_state = ckpt['model_state_dict']\n",
"elif isinstance(ckpt, dict) and all(isinstance(v, torch.Tensor) for v in ckpt.values()):\n",
"    model_state = ckpt\n",
"else:\n",
"    try:\n",
"        model_state = ckpt.get('model_state_dict', ckpt)\n",
"    except Exception:\n",
"        model_state = ckpt\n",
"\n",
"try:\n",
"    model.load_state_dict(model_state, strict=False)\n",
"    print("Model weights loaded (strict=False).")\n",
"except Exception as e:\n",
"    print("Model.load_state_dict failed:", e)\n",
"    try:\n",
"        model.load_state_dict(model_state, strict=True)\n",
"        print("Model weights loaded with strict=True (fallback).")\n",
"    except Exception as e2:\n",
"        print("Model strict load also failed:", e2)\n",
"        raise RuntimeError("Failed to load model weights.")\n",
"\n",
"model.to(device)\n",
"model.eval()\n",
"print("Model ready for inference.")\n",
"\n",
"# -----------------------\n",
"# Download and unzip images in memory\n",
"# -----------------------\n",
"print("Downloading example images zip...")\n",
"with urllib.request.urlopen(IMAGES_ZIP_URL) as response:\n",
"    zip_bytes = io.BytesIO(response.read())\n",
"\n",
"with zipfile.ZipFile(zip_bytes) as zf:\n",
"    image_files = [f for f in zf.namelist() if f.lower().endswith(('.png','.jpg','.jpeg','.tif','.tiff'))]\n",
"    if len(image_files) < 3:\n",
"        raise ValueError(f"Need at least 3 images in the zip, found {len(image_files)}")\n",
"    sampled_files = random.sample(image_files, 3)\n",
"    images = {}\n",
"    for fname in sampled_files:\n",
"        with zf.open(fname) as f:\n",
"            file_bytes = np.frombuffer(f.read(), np.uint8)\n",
"            img_bgr = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
"            images[fname] = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
"\n",
"print("Selected images:", list(images.keys()))\n",
"\n",
"# -----------------------\n",
"# Preprocess and inference\n",
"# -----------------------\n",
"def preprocess_image_for_model(orig_img_rgb, input_size):\n",
"    img = cv2.resize(orig_img_rgb, (input_size[1], input_size[0])).astype(np.float32)/255.0\n",
"    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
"    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
"    img = (img - mean)/std\n",
"    img = img.transpose(2,0,1)\n",
"    return torch.tensor(img).float().unsqueeze(0)\n",
"\n",
"def show_prediction(orig, mask, title):\n",
"    plt.figure(figsize=(12,5))\n",
"    plt.subplot(1,2,1)\n",
"    plt.imshow(orig)\n",
"    plt.title(title)\n",
"    plt.axis("off")\n",
"    plt.subplot(1,2,2)\n",
"    plt.imshow(mask, cmap='gray')\n",
"    plt.title('Predicted mask')\n",
"    plt.axis('off')\n",
"    plt.show()\n",
"\n",
"with torch.no_grad():\n",
"    for fname, img_rgb in images.items():\n",
"        h, w = img_rgb.shape[:2]\n",
"        inp = preprocess_image_for_model(img_rgb, MODEL_INPUT_SIZE).to(device)\n",
"        logits = model(inp)\n",
"        probs = torch.sigmoid(logits)[0,0].cpu().numpy()\n",
"        mask_small = (probs > THRESHOLD).astype(np.uint8) * 255\n",
"        mask = cv2.resize(mask_small, (w,h), interpolation=cv2.INTER_NEAREST)\n",
"        show_prediction(img_rgb, mask, title=fname)\n",
"\n",
"print("Done.")"
]
}
],
"metadata": {},
"nbformat": 4,
"nbformat_minor": 5
}
