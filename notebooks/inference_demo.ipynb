{
"cells": [
{
"cell_type": "code",
"execution_count": null,
"id": "checkpoint_inference_clean",
"metadata": {},
"outputs": [],
"source": [
"import io, zipfile, random, traceback\n",
"import urllib.request\n",
"import cv2\n",
"import torch\n",
"import numpy as np\n,"
"import matplotlib.pyplot as plt\n,"
"import segmentation_models_pytorch as smp\n",
"\n",
"# -----------------------\n",
"# RAW URLs\n",
"# -----------------------\n",
"WEIGHTS_URL = \"https://github.com/sequeiradiogo/Building-data-extraction/raw/main/models/unet_resnet34_epoch1_IoU0.7833.pth\"\n",
"IMAGES_ZIP_URL = \"https://github.com/sequeiradiogo/Building-data-extraction/raw/main/output/examples/test_images.zip\"\n",
"MODEL_INPUT_SIZE = (512, 512)\n",
"THRESHOLD = 0.5\n",
"\n",
"# -----------------------\n",
"# Install dependencies\n",
"# -----------------------\n",
"!pip install -q segmentation-models-pytorch==0.3.0 torchvision torch opencv-python matplotlib pillow tqdm\n",
"\n",
"# -----------------------\n",
"# Safe in-memory checkpoint loader\n",
"# -----------------------\n",
"def safe_load_checkpoint_from_bytes(weights_bytes, map_location):\n",
"    bio = io.BytesIO(weights_bytes)\n",
"    try:\n",
"        ckpt = torch.load(bio, map_location=map_location, weights_only=True)\n",
"        print(\"Loaded checkpoint with weights_only=True\")\n",
"        return ckpt\n",
"    except Exception as e:\n",
"        print(\"weights_only=True failed:\", e)\n",
"\n",
"    # fallback to trusted load\n",
"    try:\n",
"        bio.seek(0)\n",
"        ckpt = torch.load(bio, map_location=map_location, weights_only=False)\n",
"        print(\"Loaded checkpoint with weights_only=False (trusted load).\")\n",
"        return ckpt\n",
"    except Exception as e_final:\n",
"        print(\"Trusted load also failed:\", e_final)\n",
"        traceback.print_exc()\n",
"        raise RuntimeError(\"Could not load checkpoint: \" + str(e_final))\n",
"\n",
"# -----------------------\n",
"# Download model weights into memory\n",
"# -----------------------\n",
"print(\"Downloading model weights...\")\n",
"with urllib.request.urlopen(WEIGHTS_URL) as response:\n",
"    weights_bytes = response.read()\n",
"\n",
"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
"\n",
"model = smp.Unet(\n",
"    encoder_name=\"resnet34\",\n",
"    encoder_weights=None,\n",
"    in_channels=3,\n",
"    classes=1,\n",
")\n",
"\n",
"# -----------------------\n",
"# Load weights robustly\n",
"# -----------------------\n",
"ckpt = safe_load_checkpoint_from_bytes(weights_bytes, map_location=device)\n",
"\n",
"if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:\n",
"    model_state = ckpt['model_state_dict']\n",
"elif isinstance(ckpt, dict) and all(isinstance(v, torch.Tensor) for v in ckpt.values()):\n",
"    model_state = ckpt\n",
"else:\n",
"    try:\n",
"        model_state = ckpt.get('model_state_dict', ckpt)\n",
"    except Exception:\n",
"        model_state = ckpt\n",
"\n",
"model.load_state_dict(model_state, strict=False)\n",
"model.to(device)\n",
"model.eval()\n",
"print(\"Model ready for inference.\")\n",
"\n",
"# -----------------------\n",
"# Download and unzip images in memory\n",
"# -----------------------\n",
"print(\"Downloading example images zip...\")\n",
"with urllib.request.urlopen(IMAGES_ZIP_URL) as response:\n",
"    zip_bytes = io.BytesIO(response.read())\n",
"\n",
"with zipfile.ZipFile(zip_bytes) as zf:\n",
"    image_files = [f for f in zf.namelist() if f.lower().endswith(('.tif'))]\n",
"    if len(image_files) < 3:\n",
"        raise ValueError(f\"Need at least 3 images in the zip, found {len(image_files)}\")\n",
"    sampled_files = random.sample(image_files, 3)\n",
"    images = {}\n",
"    for fname in sampled_files:\n",
"        with zf.open(fname) as f:\n",
"            file_bytes = np.frombuffer(f.read(), np.uint8)\n",
"            img_bgr = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
"            images[fname] = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
"\n",
"print(\"Selected images:\", list(images.keys()))\n",
"\n",
"# -----------------------\n",
"# Preprocess and inference\n",
"# -----------------------\n",
"def preprocess_image_for_model(orig_img_rgb, input_size):\n",
"    img = cv2.resize(orig_img_rgb, (input_size[1], input_size[0])).astype(np.float32)/255.0\n",
"    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
"    std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
"    img = (img - mean)/std\n",
"    img = img.transpose(2,0,1)\n",
"    return torch.tensor(img).float().unsqueeze(0)\n",
"\n",
"def show_prediction(orig, mask, title):\n",
"    plt.figure(figsize=(12,5))\n",
"    plt.subplot(1,2,1)\n",
"    plt.imshow(orig)\n",
"    plt.title(title)\n",
"    plt.axis("off")\n",
"    plt.subplot(1,2,2)\n",
"    plt.imshow(mask, cmap='gray')\n",
"    plt.title('Predicted mask')\n",
"    plt.axis('off')\n",
"    plt.show()\n",
"\n",
"with torch.no_grad():\n",
"    for fname, img_rgb in images.items():\n",
"        h, w = img_rgb.shape[:2]\n",
"        inp = preprocess_image_for_model(img_rgb, MODEL_INPUT_SIZE).to(device)\n",
"        logits = model(inp)\n",
"        probs = torch.sigmoid(logits)[0,0].cpu().numpy()\n",
"\n",
"        mask_small = (probs > THRESHOLD).astype(np.uint8) * 255\n",
"        mask = cv2.resize(mask_small, (w,h), interpolation=cv2.INTER_NEAREST)\n",
"\n",
"        show_prediction(img_rgb, mask, title=fname)\n",
"\n",
"print(\"Done.\")"
]
}
],
"metadata": {},
"nbformat": 4,
"nbformat_minor": 5
}
